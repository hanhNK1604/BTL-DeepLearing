{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:13:42.149148Z","iopub.execute_input":"2024-06-22T06:13:42.149392Z","iopub.status.idle":"2024-06-22T06:13:57.984327Z","shell.execute_reply.started":"2024-06-22T06:13:42.149368Z","shell.execute_reply":"2024-06-22T06:13:57.983190Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport math\nimport numpy as np\n\nfrom sklearn.metrics import f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:13:57.986680Z","iopub.execute_input":"2024-06-22T06:13:57.987122Z","iopub.status.idle":"2024-06-22T06:14:05.717492Z","shell.execute_reply.started":"2024-06-22T06:13:57.987076Z","shell.execute_reply":"2024-06-22T06:14:05.716664Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n\nif tokenizer.bos_token is None:\n    tokenizer.bos_token = '<bos>'\n    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids('<bos>')\n\nif tokenizer.eos_token is None:\n    tokenizer.eos_token = '<eos>'\n    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('<eos>')\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:05.718525Z","iopub.execute_input":"2024-06-22T06:14:05.718877Z","iopub.status.idle":"2024-06-22T06:14:06.889814Z","shell.execute_reply.started":"2024-06-22T06:14:05.718854Z","shell.execute_reply":"2024-06-22T06:14:06.887309Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"239ef71d36614bc7a16e25621afb7aa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de7b4dd1008f463cb2234d6626629992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60563c8e4dd3402195e16f47a8567cbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89113134a5fe43638aceb6b46a5c3650"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"nyu-mll/glue\", \"sst2\")\n\ntrain = dataset[\"train\"]\nval = dataset[\"validation\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:06.895061Z","iopub.execute_input":"2024-06-22T06:14:06.895374Z","iopub.status.idle":"2024-06-22T06:14:11.311282Z","shell.execute_reply.started":"2024-06-22T06:14:06.895345Z","shell.execute_reply":"2024-06-22T06:14:11.310348Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b64c231f07e4ab2bfdfe40b1708adda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0889eb59520f411aaf60237819db64ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f9b27af78964672835c500217ec1f9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbbafc4d93e148ffbba04783d523a17d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e1336ad1dd4450a97adb05542647de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e830989f32904da198769f683ff1a2ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c24d879e3c74cdb839b9b9830f50499"}},"metadata":{}}]},{"cell_type":"code","source":"train_text = train['sentence']\ntrain_label = train['label']\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:11.312386Z","iopub.execute_input":"2024-06-22T06:14:11.312834Z","iopub.status.idle":"2024-06-22T06:14:11.465861Z","shell.execute_reply.started":"2024-06-22T06:14:11.312808Z","shell.execute_reply":"2024-06-22T06:14:11.464962Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_text, test_text, train_label, test_label = train_test_split(train_text, train_label, test_size=0.1, random_state=42)\ntrain_text, val_text, train_label, val_label = train_test_split(train_text, train_label, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:11.467195Z","iopub.execute_input":"2024-06-22T06:14:11.467506Z","iopub.status.idle":"2024-06-22T06:14:11.550631Z","shell.execute_reply.started":"2024-06-22T06:14:11.467479Z","shell.execute_reply":"2024-06-22T06:14:11.549962Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(len(train_text))\nprint(len(train_label))\n\nprint(len(test_text))\nprint(len(test_label))\n\nprint(len(val_text))\nprint(len(val_label))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:11.551558Z","iopub.execute_input":"2024-06-22T06:14:11.551822Z","iopub.status.idle":"2024-06-22T06:14:11.556899Z","shell.execute_reply.started":"2024-06-22T06:14:11.551798Z","shell.execute_reply":"2024-06-22T06:14:11.556001Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"54552\n54552\n6735\n6735\n6062\n6062\n","output_type":"stream"}]},{"cell_type":"code","source":"class SentimentDataset(Dataset): \n  def __init__(self, text, label, tokenizer, max_length=64): \n    self.text = text\n    self.label = label \n    self.tokenizer = tokenizer \n    self.max_length = max_length\n\n  def __len__(self): \n    return len(self.label) \n\n  def __getitem__(self, idx): \n    text_embedding = self.tokenizer.encode_plus(self.text[idx], return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)\n    label = torch.tensor(self.label[idx], dtype=torch.long) \n    return text_embedding['input_ids'].squeeze(), label\n  \n\ntrain_dataset = SentimentDataset(train_text, train_label, tokenizer)\ntest_dataset = SentimentDataset(test_text, test_label, tokenizer)\nval_dataset = SentimentDataset(val_text, val_label, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:11.558061Z","iopub.execute_input":"2024-06-22T06:14:11.558412Z","iopub.status.idle":"2024-06-22T06:14:11.569253Z","shell.execute_reply.started":"2024-06-22T06:14:11.558380Z","shell.execute_reply":"2024-06-22T06:14:11.568430Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        \n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def split_heads(self, x):\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n        \n    def combine_heads(self, x):\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        \n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        output = self.W_o(self.combine_heads(attn_output))\n        return output\n\nclass PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:11.570444Z","iopub.execute_input":"2024-06-22T06:14:11.570734Z","iopub.status.idle":"2024-06-22T06:14:11.593553Z","shell.execute_reply.started":"2024-06-22T06:14:11.570711Z","shell.execute_reply":"2024-06-22T06:14:11.592670Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class SentimentTransformer(nn.Module): \n  def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout): \n    super(SentimentTransformer, self).__init__() \n    self.vocab_size = vocab_size \n    self.d_model = d_model \n    self.num_heads = num_heads\n    self.num_layers = num_layers \n    self.d_ff = d_ff \n    self.max_seq_length = max_seq_length \n    self.dropout = nn.Dropout(dropout) \n\n    self.embedding = nn.Embedding(vocab_size, d_model) \n    self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n    self.positional_embedding = PositionalEncoding(d_model, max_seq_length) \n\n    self.fc = nn.Sequential(\n        nn.Linear(max_seq_length * d_model, d_model), \n        nn.ReLU(), \n        nn.Linear(d_model, 128), \n        nn.ReLU(), \n        nn.Linear(128, 2)\n    )\n\n  def generate_mask(self, x): \n    x_mask = (x != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2).to(x.device) \n    return x_mask \n\n  def forward(self, x): \n    x_mask = self.generate_mask(x)\n\n    x = self.embedding(x) * math.sqrt(self.d_model)\n    x = self.dropout(self.positional_embedding(x)) \n\n    for layer in self.encoder: \n      x = layer(x, x_mask) \n    \n    x = x.reshape(x.shape[0], -1)\n    x = self.fc(x) \n\n    return x\n  \n  \nx = torch.randint(size=(128, 64), low=0, high=1000) \n\nnet = SentimentTransformer(vocab_size=1000, d_model=512, num_heads=8, num_layers=4, d_ff=2048, max_seq_length=64, dropout=0.1) \n\nprint(net(x).shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:11.596421Z","iopub.execute_input":"2024-06-22T06:14:11.596744Z","iopub.status.idle":"2024-06-22T06:14:14.020811Z","shell.execute_reply.started":"2024-06-22T06:14:11.596714Z","shell.execute_reply":"2024-06-22T06:14:14.019879Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([128, 2])\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size \nd_model = 512 \nnum_heads = 8 \nnum_layers = 4 \nd_ff = 2048 \nmax_seq_length = 64 \ndropout = 0.1 \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n\nmodel = SentimentTransformer(vocab_size=vocab_size, d_model=d_model, num_heads=num_heads, num_layers=num_layers, d_ff=d_ff, max_seq_length=max_seq_length, dropout=dropout).to(device)\ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.Adam(model.parameters(), lr=1e-4) \n\nepochs = 30\n\ntrain_loss = []\nval_loss = []\n\nfor epoch in range(epochs):\n  model.train()\n  total_train_loss = 0\n  for inputs, labels in tqdm(train_loader):\n\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n    total_train_loss += loss.item()\n    train_loss.append(loss.item())\n\n  print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {total_train_loss / len(train_loader)}')\n\n  model.eval()\n  total_val_loss = 0\n  f_1 = 0\n  accuracy = 0\n  for inputs, labels in tqdm(val_loader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n\n    with torch.no_grad():\n      outputs = model(inputs)\n      loss = criterion(outputs, labels)\n      total_val_loss += loss.item()\n\n      val_loss.append(loss.item())\n\n      preds = torch.argmax(outputs, dim=-1)\n      f_1 += f1_score(labels.cpu(), preds.cpu())\n      accuracy += accuracy_score(labels.cpu(), preds.cpu())\n\n  print(f'Epoch {epoch + 1}/{epochs}, Val Loss: {total_val_loss / len(val_loader)}')\n  print(f'F1 Score: {f_1 / len(val_loader)}')\n  print(f'Accuracy: {accuracy / len(val_loader)}')\n\ntorch.save(model.state_dict(), '/kaggle/working/trans_sen.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T06:14:14.022751Z","iopub.execute_input":"2024-06-22T06:14:14.023416Z","iopub.status.idle":"2024-06-22T07:09:21.144945Z","shell.execute_reply.started":"2024-06-22T06:14:14.023379Z","shell.execute_reply":"2024-06-22T07:09:21.143755Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Train Loss: 0.5328607682536104\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Val Loss: 0.4255008703783939\nF1 Score: 0.8234147193518097\nAccuracy: 0.807079519450801\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:46<00:00,  8.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30, Train Loss: 0.3888571216241137\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30, Val Loss: 0.3592242043269308\nF1 Score: 0.848187192180916\nAccuracy: 0.8394808352402746\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:46<00:00,  8.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30, Train Loss: 0.29972599027951324\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30, Val Loss: 0.3530123156936545\nF1 Score: 0.8683066083474701\nAccuracy: 0.8607980549199084\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30, Train Loss: 0.2289699506612903\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30, Val Loss: 0.31090038183488344\nF1 Score: 0.8918560979040009\nAccuracy: 0.8820437643020596\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:46<00:00,  8.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30, Train Loss: 0.1684364954016203\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30, Val Loss: 0.32819195923052336\nF1 Score: 0.8953561029582808\nAccuracy: 0.8890517734553777\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30, Train Loss: 0.1275802036804111\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30, Val Loss: 0.2934601815907579\nF1 Score: 0.9052137625591083\nAccuracy: 0.8986842105263158\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:46<00:00,  8.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30, Train Loss: 0.10058439503219699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30, Val Loss: 0.3629246126664312\nF1 Score: 0.9027704751713334\nAccuracy: 0.8945080091533181\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30, Train Loss: 0.08767662328479053\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30, Val Loss: 0.35644027053525573\nF1 Score: 0.9048777096178404\nAccuracy: 0.896975114416476\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30, Train Loss: 0.07027770842972528\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30, Val Loss: 0.3741419283967269\nF1 Score: 0.9064745392945452\nAccuracy: 0.8993135011441648\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/30, Train Loss: 0.060449154221028055\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/30, Val Loss: 0.3883041509672215\nF1 Score: 0.9093409601985188\nAccuracy: 0.903983123569794\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/30, Train Loss: 0.059815649852944935\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/30, Val Loss: 0.4191584992565607\nF1 Score: 0.8973275376297185\nAccuracy: 0.8921410183066362\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/30, Train Loss: 0.05103839938294094\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/30, Val Loss: 0.43766663043122545\nF1 Score: 0.9088626134985301\nAccuracy: 0.9016804919908467\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/30, Train Loss: 0.04874825495096515\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/30, Val Loss: 0.41843156006775406\nF1 Score: 0.9059789624989506\nAccuracy: 0.8993778604118994\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/30, Train Loss: 0.04423649659544636\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/30, Val Loss: 0.4615880777961329\nF1 Score: 0.9106813959499461\nAccuracy: 0.9019450800915332\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/30, Train Loss: 0.04079833829552721\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/30, Val Loss: 0.3494859980125176\nF1 Score: 0.9111808847067453\nAccuracy: 0.9032608695652175\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/30, Train Loss: 0.03715730146210147\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/30, Val Loss: 0.45477123409509657\nF1 Score: 0.9079405146452322\nAccuracy: 0.8980978260869565\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/30, Train Loss: 0.03532663046428497\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/30, Val Loss: 0.49426543798885847\nF1 Score: 0.9104645053565983\nAccuracy: 0.9024027459954234\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/30, Train Loss: 0.03346743291366194\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/30, Val Loss: 0.4905975321405812\nF1 Score: 0.9060666920456941\nAccuracy: 0.8985554919908467\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/30, Train Loss: 0.03382156903084132\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/30, Val Loss: 0.40061454780791933\nF1 Score: 0.9113233200898332\nAccuracy: 0.9048698512585812\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/30, Train Loss: 0.03220881726083718\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 20.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/30, Val Loss: 0.4649682568484231\nF1 Score: 0.9100136916389095\nAccuracy: 0.9039473684210526\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/30, Train Loss: 0.029067080785829633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/30, Val Loss: 0.47387590455381495\nF1 Score: 0.9114027642841617\nAccuracy: 0.9054276315789473\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/30, Train Loss: 0.026154608260562786\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/30, Val Loss: 0.5047960391562236\nF1 Score: 0.9143072691209402\nAccuracy: 0.9075014302059496\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/30, Train Loss: 0.025450740994095087\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/30, Val Loss: 0.5511685527076847\nF1 Score: 0.9102973450562714\nAccuracy: 0.9018092105263158\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/30, Train Loss: 0.027582643342091526\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/30, Val Loss: 0.44269589460209796\nF1 Score: 0.9099058107573348\nAccuracy: 0.903854405034325\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/30, Train Loss: 0.02405894566770203\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/30, Val Loss: 0.461241626739502\nF1 Score: 0.9125397454766027\nAccuracy: 0.9058924485125859\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/30, Train Loss: 0.024756302184949095\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/30, Val Loss: 0.4802042990922928\nF1 Score: 0.9103876469150554\nAccuracy: 0.9029319221967964\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/30, Train Loss: 0.02245221776055398\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/30, Val Loss: 0.5889129248888869\nF1 Score: 0.9108428888384666\nAccuracy: 0.9035540617848971\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/30, Train Loss: 0.022854512333990717\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/30, Val Loss: 0.5641910691010324\nF1 Score: 0.910649683029494\nAccuracy: 0.9034897025171625\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/30, Train Loss: 0.023007016552553227\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/30, Val Loss: 0.5775966953290136\nF1 Score: 0.9060202174732647\nAccuracy: 0.9001358695652174\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 853/853 [01:45<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/30, Train Loss: 0.021727376544953447\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95/95 [00:04<00:00, 21.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/30, Val Loss: 0.5406996479944179\nF1 Score: 0.9086369740420516\nAccuracy: 0.9020809496567505\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad(): \n    f_1 = 0\n    accuracy = 0\n    for inputs, labels in tqdm(test_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        preds = torch.argmax(outputs, dim=-1) \n        f_1 += f1_score(labels.cpu(), preds.cpu())\n        accuracy += accuracy_score(labels.cpu(), preds.cpu())\n        \n    print(f'F1 score on test set: {f_1/len(test_loader)}')\n    print(f'Accuracy on test set: {accuracy/len(test_loader)}')\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:10:22.269639Z","iopub.execute_input":"2024-06-22T07:10:22.270587Z","iopub.status.idle":"2024-06-22T07:10:27.354678Z","shell.execute_reply.started":"2024-06-22T07:10:22.270553Z","shell.execute_reply":"2024-06-22T07:10:27.353721Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 106/106 [00:05<00:00, 20.89it/s]","output_type":"stream"},{"name":"stdout","text":"F1 score on test set: 0.9129823645282286\nAccuracy on test set: 0.9029677672955975\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}